{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "6642715e",
   "metadata": {},
   "source": [
    "# Information Retrieval Week 6 Tutorial\n",
    "\n",
    "##### version 1.1\n",
    "\n",
    "###### INFS7410 team\n",
    "\n",
    "---\n",
    "\n",
    "##### About today's tutorial\n",
    "\n",
    "In this week's tutorial, you will be learning about and implementing a pointwise Learning to Rank (LTR) method. LTR refers to the application of machine learning to the ranking problem.\n",
    "\n",
    "##### Tutorial Etiquette\n",
    "Please refrain from loud noises, irrelevant conversations and use of mobile phones during tutorial activities. Be respectful of everyone's opinions and ideas during the tutorial activities. You will be asked to leave if you disturb. Remember the tutor is there to help you understand and learn, not to provide debugging of your code or solutions to assignments. \n",
    "\n",
    "\n",
    "## Exercise 1: \n",
    "The Learning to Rank dataset you are going to use in this execise is the MQ2007 dataset, which is a subset of LETOR: a package of benchmark datasets for research on LEarning TO Rank.[You can find details of this collection here](https://arxiv.org/pdf/1306.2597.pdf).\n",
    "Description of the MQ2007 dataset: Each row refers to a query-document pair. The first column is the relevance label of this pair, the second column is the query id, the following columns are the features that represent the document (often in the context of the query), and the end of the row is a comment about the pair, which in the case of this dataset includes the id of the document. The larger the relevance label, the more relevant the document is to the query. In the MQ20007 dataset, a query-document pair is represented by a 46-dimensional feature vector. The features used in this dataset include TF, TF-IDF, BM25 and LM scores. \n",
    "\n",
    "Logistic regression is a probabilistic classification task which assigns labels to classes according to a hypothesis function. In order to learn a classifier, the hypothesis function is used to update *weights* assigned to the features of the classes to be classified.\n",
    "\n",
    "There are many different LTR techniques. In this practical we use logistic regression.  Here, the classifier becomes  the ranker used to order retrieval results. Specifically, in this exercise, you will learn how to implement a logistic regression to train a linear ranker.\n",
    "\n",
    "Here are several example rows from the MQ2007 dataset:\n",
    "\n",
    "---\n",
    "2 qid:10032 1:0.056537 2:0.000000 3:0.666667 4:1.000000 5:0.067138 ... 45:0.000000 46:0.076923 #docid=GX029-35-5894638 inc=0.0119881192468859 prob=0.139842\n",
    "\n",
    "0 qid:10032 1:0.279152 2:0.000000 3:0.000000 4:0.000000 5:0.279152 ... 45:0.250000 46:1.000000 #docid=GX030-77-6315042 inc=1 prob=0.341364\n",
    "\n",
    "0 qid:10032 1:0.130742 2:0.000000 3:0.333333 4:0.000000 5:0.134276 ... 45:0.750000 46:1.000000 #docid=GX140-98-13566007 inc=1 prob=0.0701303\n",
    "\n",
    "1 qid:10032 1:0.593640 2:1.000000 3:0.000000 4:0.000000 5:0.600707 ... 45:0.500000 46:0.000000 #docid=GX256-43-0740276 inc=0.0136292023050293 prob= 0.400738\n",
    "\n",
    "---\n",
    "\n",
    "Note: the relavence labels of this dataset are graded from 0 (non-relevant) to 2 (highly relevent); however we binarise them for convinience, i.e. transform the graded relevance to binary relevance (0 stays 0, while 1 and 2 become 1).\n",
    "\n",
    "Now we can treat the ranking task as a binary classification problem (thus becoming a LTR problem): given an input, we want to predict the output. In our case, the input is a feature vector (which for MQ2007 contains 46 dimensions), while the output is the (binary) relevance label (0 or 1)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "80a9a5f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import math\n",
    "\n",
    "# ------------------------------------------------\n",
    "\n",
    "class Dataset():\n",
    "    \"\"\"Representation of the LTR datasets.\"\"\"\n",
    "    def __init__(self, file_path, num_features):\n",
    "        self.num_features = num_features\n",
    "        self.label_vector = []\n",
    "        self.feature_matrix = []\n",
    "        self.qid_all_features = {}\n",
    "        self.qid_all_docids = {}\n",
    "        self.qid_relevance_set = {}\n",
    "        self.qid_docid_relevance = {}   \n",
    "        \n",
    "        with open(file_path, \"r\") as f:\n",
    "            data = f.readlines()\n",
    "            \n",
    "        docids = []\n",
    "        query_features = []\n",
    "        relevance_set = []\n",
    "        docid_rel = {}\n",
    "        current_qid = None\n",
    "        for line in data:\n",
    "            tokens = line.split()\n",
    "            qid = tokens[1].split(\":\")[1]\n",
    "            \n",
    "            if qid != current_qid and current_qid is not None:\n",
    "                self.qid_all_docids[current_qid] = docids.copy()\n",
    "                self.qid_all_features[current_qid] = query_features.copy()\n",
    "                self.qid_relevance_set[current_qid] = relevance_set.copy()\n",
    "                self.qid_docid_relevance[current_qid] = docid_rel.copy()\n",
    "                docids = []\n",
    "                query_features = []\n",
    "                relevance_set = []\n",
    "                docid_rel = {}\n",
    "                \n",
    "            current_qid = qid\n",
    "            \n",
    "            docid = tokens[self.num_features+4]\n",
    "            relevance = int(tokens[0])\n",
    "            if relevance > 0:\n",
    "                relevance = 1\n",
    "            self.label_vector.append(relevance)\n",
    "            \n",
    "            features = []\n",
    "            for i in range(self.num_features):\n",
    "                feature = float(tokens[i+2].split(\":\")[1])\n",
    "                features.append(feature)\n",
    "            \n",
    "            self.feature_matrix.append(features)\n",
    "            \n",
    "            docids.append(docid)\n",
    "            query_features.append(features)\n",
    "            relevance_set.append(relevance)\n",
    "            docid_rel[docid] = relevance\n",
    "    \n",
    "    def relevance_set(self, qid):\n",
    "        return self.qid_relevance_set[qid].copy()\n",
    "\n",
    "# ------------------------------------------------    \n",
    "    \n",
    "def ndcg_at_k(dataset, query_result_list, k):\n",
    "    \"\"\"Evaluation measure we will use.\"\"\"\n",
    "    ndcg = 0.0\n",
    "    num_query = 0.0\n",
    "    \n",
    "    for qid in query_result_list.keys():\n",
    "        result_list_size = len(query_result_list[qid])\n",
    "        if result_list_size < k:\n",
    "            k = result_list_size\n",
    "            \n",
    "        dcg = 0.0\n",
    "        for i in range(k):\n",
    "            docid = query_result_list[qid][i]\n",
    "            relevance = dataset.qid_docid_relevance[qid][docid]\n",
    "            dcg += math.pow(2, relevance) / (math.log(i+2)/math.log(2))\n",
    "            \n",
    "        rel_set = dataset.relevance_set(qid)\n",
    "        rel_set = sorted(rel_set, reverse=True)\n",
    "        \n",
    "        idcg = 0.0\n",
    "        for i in range(k):\n",
    "            idcg += math.pow(2, rel_set[i]) / (math.log(i+2)/math.log(2))\n",
    "        ndcg += dcg/idcg\n",
    "        num_query += 1\n",
    "    ndcg /= num_query\n",
    "    return ndcg\n",
    "\n",
    "# ------------------------------------------------\n",
    "num_features = 46 # Dimentionality of the data.\n",
    "num_epoch = 10 # How many epochs to train for.\n",
    "train_dataset = Dataset(\"train.txt\", num_features)\n",
    "test_dataset = Dataset(\"test.txt\", num_features)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d95e3e91",
   "metadata": {},
   "source": [
    "### Task 1\n",
    "\n",
    "First, you need to implement `compute_hypothesis` method in `LinearRanker` class below:\n",
    "\n",
    "The training set $X$ is the set of all training examples (represented as a matrix):\n",
    "$$\n",
    "X=x\\in\n",
    "\\begin{bmatrix}\n",
    "x^0\\\\\n",
    "x^1\\\\\n",
    "...\\\\\n",
    "x^n\n",
    "\\end{bmatrix}\n",
    "$$\n",
    "Where $x^i$ is the ith training example in the training set which represented by a feature vector. Your task is the implement the hypothesis function which attempts to predict the relevance label for a given training example. The function should return a number between 0 and 1 where 0 is non-relevance and 1 is relevant.\n",
    "$$\n",
    "h_\\theta(x) = \\frac{1}{1+e^{-\\theta^{T}x}}\n",
    "$$\n",
    "Where $\\theta$ is the rankers' feature weights and $x$ is the training example (a single line in the training data). "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "188d8dce",
   "metadata": {},
   "source": [
    "### Task 2\n",
    "\n",
    "Second, implement `compute_gradient` and `update_weights` methods in LinearRanker class below. We have provided a skeleton for the function. \n",
    "\n",
    "In `compute_gradient` function, you need to write code to compute update gradient for each weight in the `LinearRanker`, the equation is following:\n",
    "\n",
    "$$\n",
    "\\nabla\\theta_j=\\frac{1}{m}\\sum_{i=1}^{m}(h_\\theta(x^i)-y^i)\\cdot x_{j}^i\n",
    "$$\n",
    "where $m$ is the number of rows in the training set, $x^{i}$ the $i^{\\text{th}}$ row in the training set, $y^{i}$ is the relevance label of the $i^{\\text{th}}$ row, and $x_{j}^{i}$ is a single feature in the $i^{\\text{th}}$ row.\n",
    "\n",
    "After you implemente the `compute_gradient` method, you can update ranker's weights in `update_weights` method by:\n",
    "\n",
    "$$\n",
    "\\theta_j=\\theta_j-\\alpha\\cdot\\nabla\\theta_j\n",
    "$$\n",
    "\n",
    "Where $\\alpha$ is the learning rate which has been fixed to a default value 0.1 (feel free to change it!). \n",
    "\n",
    "If implemented correctly, you will see the nDCG@10 increasing gradually each epoch.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "id": "f0ee417e",
   "metadata": {},
   "outputs": [],
   "source": [
    "class LinearRanker():\n",
    "    \n",
    "    def __init__(self, num_features):\n",
    "        self.num_features = num_features\n",
    "        self.weight_vector = np.random.normal(loc=0.0, scale=1.0, size=num_features)\n",
    "        \n",
    "    def compute_hypothesis(self, X):\n",
    "        \"\"\"Compute sigmoid hypothesis function.\"\"\"\n",
    "        # Your implementation here!\n",
    "        \n",
    "        score = 1/(1 + np.exp(-(np.mat(self.weight_vector) * np.mat(X).T)))\n",
    "        return score[0][0]\n",
    "    \n",
    "    def compute_gradient(self, feature_matrix, label_vector, weight_index):\n",
    "        \"\"\"Compute gradient for given feature weight index.\"\"\"\n",
    "        # Your implementation here!\n",
    "        \n",
    "        gradient = 0\n",
    "        for i in range(0, len(feature_matrix)):\n",
    "            gradient = gradient + (((self.compute_hypothesis(feature_matrix[i]) - label_vector[i])) * feature_matrix[i][weight_index])\n",
    "        gradient = gradient/self.num_features\n",
    "        return gradient\n",
    "\n",
    "    def update_weights(self, learning_rate, feature_matrix, label_vector):\n",
    "        \"\"\"Update ranker's feature weights.\"\"\"\n",
    "        # Your implementation here!\n",
    "        \n",
    "        for i in range(0, self.num_features):\n",
    "            temp= self.weight_vector[i]\n",
    "            self.weight_vector[i] = temp - learning_rate * self.compute_gradient(feature_matrix, label_vector, i)\n",
    "    \n",
    "    def compute_scores(self, feature_list):\n",
    "        scores = []\n",
    "        for features in feature_list:\n",
    "            score = 0\n",
    "            for i in range(self.num_features):\n",
    "                score += features[i] * self.weight_vector[i]\n",
    "            scores.append(score)\n",
    "        return scores\n",
    "\n",
    "    def get_query_result_list(self, dataset):\n",
    "        query_result_set = {}\n",
    "        for qid in dataset.qid_all_docids.keys():\n",
    "            doc_list = dataset.qid_all_docids[qid].copy()\n",
    "            feature_list = dataset.qid_all_features[qid].copy()\n",
    "            scores = self.compute_scores(feature_list)\n",
    "            \n",
    "            mapping = []\n",
    "            for i in range(len(doc_list)):\n",
    "                mapping.append((doc_list[i], scores[i]))\n",
    "            mapping = sorted(mapping, reverse=True, key=lambda x: x[1])\n",
    "            \n",
    "            result_list = [x[0] for x in mapping]\n",
    "            query_result_set[qid] = result_list\n",
    "        return query_result_set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "id": "01ac79dd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ndcg@10: 0.7500254818562296 after 1/10 epochs\n",
      "ndcg@10: 0.7179380251787845 after 2/10 epochs\n",
      "ndcg@10: 0.7391110394118868 after 3/10 epochs\n",
      "ndcg@10: 0.7560727982341146 after 4/10 epochs\n",
      "ndcg@10: 0.7557547599756669 after 5/10 epochs\n",
      "ndcg@10: 0.7419141032261893 after 6/10 epochs\n",
      "ndcg@10: 0.7377112214276289 after 7/10 epochs\n",
      "ndcg@10: 0.7392545556464268 after 8/10 epochs\n",
      "ndcg@10: 0.7403830935829396 after 9/10 epochs\n",
      "ndcg@10: 0.7352054942361922 after 10/10 epochs\n"
     ]
    }
   ],
   "source": [
    "# Run this cell once you have implemented the methods above!\n",
    "ranker = LinearRanker(num_features)\n",
    "for i in range(num_epoch):\n",
    "    ranker.update_weights(0.5, train_dataset.feature_matrix, train_dataset.label_vector)\n",
    "    ndcg_at_10 = ndcg_at_k(test_dataset, ranker.get_query_result_list(test_dataset), 10)\n",
    "    print(f\"ndcg@10: {ndcg_at_10} after {i+1}/{num_epoch} epochs\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "87188545",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
