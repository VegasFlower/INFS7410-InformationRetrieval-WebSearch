{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "a0773a3c",
   "metadata": {},
   "source": [
    "# INFS7410 Week 5 Practical \n",
    "\n",
    "##### version 1.0\n",
    "\n",
    "###### The INFS7410 Teaching Team\n",
    "\n",
    "##### Tutorial Etiquette\n",
    "Please refrain from loud noises, irrelevant conversations and use of mobile phones during tutorial activities. Be respectful of everyone's opinions and ideas during the tutorial activities. You will be asked to leave if you disturb. Remember the tutor is there to help you understand and learn, not to provide debugging of your code or solutions to assignments. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f494f251",
   "metadata": {},
   "source": [
    "#### About today's tutorial\n",
    "In this week's tutorial, you will be learning about and implementing methods for query expansion and reduction using relevance feedback."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d039df0b",
   "metadata": {},
   "source": [
    "----\n",
    "## Exercise 1: Pseudo-relevance Feedback Query Expansion\n",
    "\n",
    "When discussing the Binary Independence Model and BM25, we noticed these models allowed for taking into account information about relevant (and not relevant) documents. Similarly, we could adapt other models to make use of statistics from relevant documents. The problem, however, is that in most cases we start with a query, but no relevant documents.\n",
    "\n",
    "The intuition of pseudo-relevance feedback (PRF) is to assume that the top k documents retrieved by the search engine in answer to the query are relevant. Then, statistics from these documents can be used as if they were from relevant documents, to re-perform retrieval.\n",
    "\n",
    "We can take this a step further, and instead of simply performing term-reweighting for the original query, we can use the pseudo-relevance feedback document to actually augment the query with additional query terms -- in a hope to improve the retrieval effectiveness over running the original query. This is called pseudo-relevance feedback query expansion, and it works as follows:\n",
    "\n",
    "1. rank documents using the original query.\n",
    "2. consider the top n documents only.\n",
    "3. rank terms in those documents by a weighting scheme: for example we will use tf-idf to rank terms.\n",
    "4. add the top m terms from the top n documents to the original query.\n",
    "\n",
    "Note that you can use this mechanism by combining any retrieval model for ranking documents and any retrieval model/weighting schema to rank terms.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "88c9dbab",
   "metadata": {},
   "source": [
    "First, let's load some packages and tool functions you will need in this practical. You have seen most of them before."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "18d7cbb5",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyserini.search import SimpleSearcher\n",
    "from pyserini.analysis import Analyzer, get_lucene_analyzer\n",
    "import pytrec_eval\n",
    "from pyserini.index import IndexReader\n",
    "import math\n",
    "from collections import Counter\n",
    "\n",
    "\n",
    "\n",
    "searcher = SimpleSearcher('indexes/lucene-index-msmarco-passage-vectors-noProcessing/')\n",
    "index_reader = IndexReader('indexes/lucene-index-msmarco-passage-vectors-noProcessing/')\n",
    "lucene_analyzer = get_lucene_analyzer(stemming=False, stopwords=False)\n",
    "analyzer = Analyzer(lucene_analyzer)\n",
    "searcher.set_analyzer(lucene_analyzer)\n",
    "\n",
    "queries = []\n",
    "with open(\"queries.tsv\", \"r\") as f:\n",
    "    for line in f.readlines():\n",
    "        parts = line.split(\"\\t\")\n",
    "        # parts[0] ~> topic id\n",
    "        # parts[1] ~> query\n",
    "        queries.append((parts[0], parts[1].strip()))\n",
    "        \n",
    "def search(run_file: str, k: int=1000):\n",
    "    with open(run_file, \"w\") as f:\n",
    "        for topic_id, query in queries: # topic id is important here.\n",
    "            hits = searcher.search(query, k=k)\n",
    "            for i, hit in enumerate(hits):\n",
    "                # Write the results to our file.\n",
    "                f.write(f\"{topic_id} Q0 {hit.docid} {i} {hit.score} infs7410_w5\\n\")\n",
    "                \n",
    "def print_results(run_file, qrel_file='qrel.txt', measures=[\"map\", \"ndcg_cut_10\", \"recall_1000\"]):\n",
    "    with open(run_file, \"r\") as f:\n",
    "        run = pytrec_eval.parse_run(f)\n",
    "    # Open the qrels file.\n",
    "    with open(qrel_file, \"r\") as f:\n",
    "        msmarco_qrels = pytrec_eval.parse_qrel(f)\n",
    "\n",
    "    evaluator = pytrec_eval.RelevanceEvaluator(query_relevance=msmarco_qrels, measures=measures)\n",
    "    results = evaluator.evaluate(run)\n",
    "    for measure in sorted(measures):\n",
    "        print('{:25s}{:8s}{:.4f}'.format(measure, 'all', pytrec_eval.compute_aggregated_measure(measure,\n",
    "                                  [query_measures[measure]for query_measures in results.values()]))) \n",
    "        \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1e11c4d1",
   "metadata": {},
   "source": [
    "Now let's first get a baseline run that does not apply any query modifications. For this, we use pyserini SimpleSearcher API which uses BM25 as default retrieval model:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "2a278559",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "map                      all     0.3566\n",
      "ndcg_cut_10              all     0.4854\n",
      "recall_1000              all     0.6881\n"
     ]
    }
   ],
   "source": [
    "search(\"prac5_k1000.run\", k=1000)\n",
    "print_results('prac5_k1000.run')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d1cca473",
   "metadata": {},
   "source": [
    "Remember these evaluation scores which we will use to compare query expansion and reduction later."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7605c2d5",
   "metadata": {},
   "source": [
    "## Your task for this exercise\n",
    "\n",
    "Implement the below function that does pseudo-relevance feedback query expansion.\n",
    "\n",
    "As an reminder, follow this algorithm:\n",
    "\n",
    "1. rank documents using the original query. (we already done this for you, that is `hits = searcher.search(query, k=50)`)\n",
    "2. consider the top n documents only.\n",
    "3. rank terms in those documents by a weighting scheme: for example we will use tf-idf to rank terms.\n",
    "4. add the top m terms from the top n documents to the original query.\n",
    "\n",
    "`prf_query_expansion` takes original `query` string, `n` and `m` as inputs, and outputs the expanded query string.\n",
    "\n",
    "Hints: \n",
    "\n",
    "- You may want to use `index_reader.get_document_vector(docid)` to get term frequence in the given document.\n",
    "- You may want to use `index_reader.get_term_counts(term, analyzer=None)[0]` to get term document frequence in the collection."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "f28e4091",
   "metadata": {},
   "outputs": [],
   "source": [
    "N = index_reader.stats()[\"documents\"]\n",
    "\n",
    "def prf_query_expansion(query: str, n: int, m: int):\n",
    "    hits = searcher.search(query, k=50)\n",
    "\n",
    "    # TODO\n",
    "    #print(query)\n",
    "    rank = {}\n",
    "    q_terms = analyzer.analyze(query)\n",
    "\n",
    "    for i in range(0, n):\n",
    "        tf = index_reader.get_document_vector(hits[i].docid)\n",
    "        df = {term: (index_reader.get_term_counts(term, analyzer=None))[0] for term in tf.keys()}\n",
    "        \n",
    "        #c = list((Counter(q_terms) & Counter(tf.keys())).elements())\n",
    "        #c = Counter(tf.keys()).elements()\n",
    "        c = list((Counter(tf.keys()) - Counter(q_terms)).elements())\n",
    "\n",
    "\n",
    "        #print(c)\n",
    "\n",
    "        for term in c:\n",
    "            tfidf = (tf[term]*math.log(N/(1+df[term])))\n",
    "            if term not in rank:\n",
    "                rank[term] = tfidf\n",
    "            else:\n",
    "                rank[term] += tfidf\n",
    "    #print(rank)\n",
    "    \n",
    "    sorted_rank = dict(sorted(rank.items(), key=lambda item: item[1], reverse=True))\n",
    "\n",
    "    expanded_query = query\n",
    "    \n",
    "    top_terms = list(sorted_rank.keys())[:m]\n",
    "    \n",
    "    #print(top_terms)\n",
    "    \n",
    "    for i in top_terms:\n",
    "        expanded_query += ' '\n",
    "        expanded_query += i\n",
    "\n",
    "        #print(expanded_query)\n",
    "    \n",
    "    #print(expanded_query)\n",
    "    \n",
    "    return expanded_query"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f75540ad",
   "metadata": {},
   "source": [
    "Run the following sell to test your `prf_query_expansion`, does the added terms make sense to you? feel free to change parameters and the original query to play around."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "35d2cba0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original query: what is information retrieval?\n",
      "Expanded query: what is information retrieval? metadata encoding searching query indexing overload ir improve text process\n"
     ]
    }
   ],
   "source": [
    "original_query = \"what is information retrieval?\"\n",
    "expanded_query = prf_query_expansion(original_query, 5, 10)\n",
    "print(f\"Original query: {original_query}\")\n",
    "print(f\"Expanded query: {expanded_query}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eaf2914f",
   "metadata": {},
   "source": [
    "If you happy with your `prf_query_expansion`, then run the new search function blow to evaluate your `prf_query_expansion`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "id": "111e4db3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "map                      all     0.3340\n",
      "ndcg_cut_10              all     0.4711\n",
      "recall_1000              all     0.7351\n"
     ]
    }
   ],
   "source": [
    "def prf_expansion_search(run_file: str, k: int=1000, n: int=5, m: int=10):\n",
    "    with open(run_file, \"w\") as f:\n",
    "        for topic_id, query in queries: # topic id is important here.\n",
    "            expanded_query = prf_query_expansion(query, n, m)\n",
    "            hits = searcher.search(expanded_query, k=k)\n",
    "            for i, hit in enumerate(hits):\n",
    "                # Write the results to our file.\n",
    "                f.write(f\"{topic_id} Q0 {hit.docid} {i} {hit.score} infs7410_w5\\n\")\n",
    "                \n",
    "prf_expansion_search(\"prac5_prf_exp_k1000.run\", k=1000, n=10, m=1)\n",
    "print_results('prac5_prf_exp_k1000.run')\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# (30,3) 33 43 73\n",
    "\n",
    "# (20,20)29 40 71 \n",
    "# (20,10)32 42 72\n",
    "# (20,5) 31 42 74\n",
    "# (20,3) 34 44 73\n",
    "# (20,2) 35 45 75\n",
    "\n",
    "# (10,10)30 43 70\n",
    "# (10,5) 31 42 71\n",
    "# (10,3) 30 41 71\n",
    "# (10,1) 33 47 74\n",
    "\n",
    "# (5,10) 29 45 68\n",
    "# (5,5)  31 43 70\n",
    "# (3,3)  31 44 66"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cf93f2c6",
   "metadata": {},
   "source": [
    "**QUESTION:** _Compare to the metrics' score with no prf expansion, which metric increased and which metric decreased? Why this is the case?_"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a9a55714",
   "metadata": {},
   "source": [
    "----\n",
    "## Exercise 2: IDF-r Query Reduction\n",
    "\n",
    "So far we have considered methods to improve the representation of the query by adding terms. Next, we consider the case of removing terms from the query, to make the query more focused. The removal of terms brings also other advantages such as less terms for which to iterate through the postings -- thus a faster query processing: this is important in particular for large queries.\n",
    "\n",
    "In this exercise we consider a simple query reduction approach in which terms are ranked by IDF score, and then the top n terms (i.e., those with the highest IDF, that is, the most discriminative), are selected.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8b2ad5d4",
   "metadata": {},
   "source": [
    "## Your task for this exercise\n",
    "\n",
    "Similar to the previous exercise, implement the `idfr_query_reduction` in the following cell:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "id": "18b81301",
   "metadata": {},
   "outputs": [],
   "source": [
    "N = index_reader.stats()[\"documents\"]\n",
    "\n",
    "def idfr_query_reduction(query: str, n: int):\n",
    "    terms = analyzer.analyze(query)\n",
    "    #print(terms)\n",
    "\n",
    "\n",
    "    terms = list(dict.fromkeys(terms))\n",
    "    # TODO\n",
    "    \n",
    "    #print(terms)\n",
    "    \n",
    "    idf = {}\n",
    "    \n",
    "    df = {term: (index_reader.get_term_counts(term, analyzer=None))[0] for term in terms}\n",
    "    \n",
    "    for term in terms:\n",
    "        term_idf = math.log(N/(1+df[term]))\n",
    "        idf[term] = term_idf\n",
    "    \n",
    "    sorted_idf = dict(sorted(idf.items(), key=lambda item: item[1], reverse=True))\n",
    "    #print(sorted_idf)\n",
    "\n",
    " \n",
    "    top_idf = list(sorted_idf.keys())[:n]\n",
    "    \n",
    "    #print(top_idf)\n",
    "    \n",
    "    #result = terms\n",
    "    \n",
    "    #for term in top_idf:\n",
    "    #    result.remove(term)\n",
    "    \n",
    "    #print(result)\n",
    "    \n",
    "    pruned_query = ' '.join(top_idf)\n",
    "    \n",
    "    \n",
    "    return pruned_query"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "56d7541e",
   "metadata": {},
   "source": [
    "Run the following cell to test your `idfr_query_reduction`. As IDF-r reduction is usually applied for long queries, so we make up a relatively long query to test. Again you can play around with the parameter query."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "id": "9ff0a3a1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original query: what is information retrieval? what it is used for? and how can it help us to access knowledge?\n",
      "Pruned query: retrieval knowledge access us help information what how used can\n"
     ]
    }
   ],
   "source": [
    "original_query = \"what is information retrieval? what it is used for? and how can it help us to access knowledge?\"\n",
    "pruned_query = idfr_query_reduction(original_query, 10)\n",
    "print(f\"Original query: {original_query}\")\n",
    "print(f\"Pruned query: {pruned_query}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "efb52e72",
   "metadata": {},
   "source": [
    "You may find that set `n` to be a large number means you will keep all the query terms thus won't change the original query at all. In addition to setting n to be a fixed number, you can also try to set it as a ratio to determine the percentage of the entire term set you want to keep, for example, `n=0.8` means keeping 80% of the query terms."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d988f112",
   "metadata": {},
   "source": [
    "Finally, run the following cell to evaluate how your `idfr_query_reduction`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "id": "552776a2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "map                      all     0.3565\n",
      "ndcg_cut_10              all     0.4854\n",
      "recall_1000              all     0.6881\n"
     ]
    }
   ],
   "source": [
    "def idfr_reduction_search(run_file: str, k: int=1000, n: int=10):\n",
    "    with open(run_file, \"w\") as f:\n",
    "        for topic_id, query in queries: # topic id is important here.\n",
    "            pruned_query = idfr_query_reduction(query, n)\n",
    "            hits = searcher.search(pruned_query, k=k)\n",
    "            for i, hit in enumerate(hits):\n",
    "                # Write the results to our file.\n",
    "                f.write(f\"{topic_id} Q0 {hit.docid} {i} {hit.score} infs7410_w5\\n\")\n",
    "                \n",
    "idfr_reduction_search(\"prac5_idfr_k1000.run\", k=1000, n=8)\n",
    "print_results('prac5_idfr_k1000.run')\n",
    "\n",
    "\n",
    "# n=3 0.3558 0.4666 0.6801\n",
    "# n=5 0.3544 0.4805 0.6879\n",
    "# n=8 0.3565 0.4854 0.6881"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a2c3191a",
   "metadata": {},
   "source": [
    "**QUESTION:** _Compare to the metrics' score with no prf expansion and with prf expansion, which metric increased and which metric decreased? Why this is the case?_"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aee1535b",
   "metadata": {},
   "source": [
    "-----\n",
    "## Challenge exercise\n",
    "\n",
    "Create and evaluate a search function that combines prf query expansion and IDFr query reduction. \n",
    "\n",
    "You can try: prf_expansion --> IDFr_reduction or IDFr_reduction --> prf_expansion."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d234a889",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
