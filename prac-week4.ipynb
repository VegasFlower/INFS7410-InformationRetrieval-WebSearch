{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "341be18a",
   "metadata": {},
   "source": [
    "# INFS7410 Week 4 Practical \n",
    "\n",
    "##### version 1.0\n",
    "\n",
    "###### The INFS7410 Teaching Team"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7bbc9703",
   "metadata": {},
   "source": [
    "##### Tutorial Etiquette\n",
    "Please refrain from loud noises, irrelevant conversations and use of mobile phones during tutorial activities. Be respectful of everyone's opinions and ideas during the tutorial activities. You will be asked to leave if you disturb. Remember the tutor is there to help you understand and learn, not to provide debugging of your code or solutions to assignments. \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "23611ae6",
   "metadata": {},
   "source": [
    "## Exercise 1: \n",
    "\n",
    "Below, you have been provided some sample Python code with partial implementations for two rank fusion algorithms -- Borda and CombSUM. Your task in this exercise is to complete these partially implemented algorithms, then implement from scratch another algorithm called CombMNZ, and finally perform an analysis of the results of these algorithms.\n",
    "\n",
    "Rank fusion (also called ranking fusion or runs fusion) algorithms aim to leverage two or more document rankings returned in answer to a query to produce a new ranking that is formed by combining the rankings provided as input. This fusion can be interpreted and understood using a variety of metaphors. For example, you can think that each individual ranking, or run, expresses a vote towards the documents it contains -- and these votes could be weighted by the rank positions. Then, fusing together $k$ runs could be interpreted as an election process, where the rank fusion algorithm is talling together the votes for the different documents expressed by the individual rankings. This is for example the metaphor used by Borda; the open question is: how do we count the votes, and consider the weights with respect to the rank position? This tutorial will provide you with an intuition for some of the rank fusion approaches; we will see more details as usual in the lecture.\n",
    "\n",
    "You can find a good description of rank fusion algorithms in: Benham & Culpepper, _Risk-reward trade-offs in rank fusion_, ADCS'17. (note, our notation slightly differs from theirs to make things clearer)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "75f8005b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import minmax_scale\n",
    "from collections import defaultdict\n",
    "import pytrec_eval\n",
    "import numpy as np\n",
    "from pprint import pprint\n",
    "\n",
    "def print_results(run, qrel_file='qrel.txt', measures=[\"map\", \"ndcg_cut_10\", \"recall_1000\"]):\n",
    "    # Open the qrels file.\n",
    "    with open(qrel_file, \"r\") as f:\n",
    "        msmarco_qrels = pytrec_eval.parse_qrel(f)\n",
    "\n",
    "    evaluator = pytrec_eval.RelevanceEvaluator(query_relevance=msmarco_qrels, measures=measures)\n",
    "    results = evaluator.evaluate(run)\n",
    "    for measure in sorted(measures):\n",
    "        print('{:25s}{:8s}{:.4f}'.format(measure, 'all', pytrec_eval.compute_aggregated_measure(measure,\n",
    "                                  [query_measures[measure]for query_measures in results.values()])))  \n",
    "        \n",
    "def normalise_run(run):\n",
    "    for k, v in run.items():\n",
    "        #print(k,v)\n",
    "        r = [(docid, score) for docid, score in v.items()]\n",
    "        scores = minmax_scale([x[1] for x in r])\n",
    "        run[k] = dict(zip([x[0] for x in r], scores))\n",
    "    return run"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "03754c89",
   "metadata": {},
   "source": [
    "In this practical, you will use the run files you created in the previous week, and apply rank fusion to them. This means that you need to go back and complete last weeks prac if you have not yet done so."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "da8691a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"week3-tf.run\", \"r\") as f:\n",
    "    week3_tf_run = normalise_run(pytrec_eval.parse_run(f))\n",
    "    \n",
    "with open(\"week3-idf.run\", \"r\") as f:\n",
    "    week3_idf_run = normalise_run(pytrec_eval.parse_run(f))\n",
    "    \n",
    "with open(\"week3-tfidf.run\", \"r\") as f:\n",
    "    week3_tfidf_run = normalise_run(pytrec_eval.parse_run(f))\n",
    "\n",
    "with open(\"week3-bm25.run\", \"r\") as f:\n",
    "    week3_bm25_run = normalise_run(pytrec_eval.parse_run(f))\n",
    "    \n",
    "runs = [week3_tf_run, week3_idf_run, week3_tfidf_run, week3_bm25_run]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6dec6f94",
   "metadata": {},
   "source": [
    "### Borda\n",
    "\n",
    "Borda count is a voting algorithm that sums the difference in rank position from the total number of document candidates in each list, or ranking. It can be expressed formally as:\n",
    "$$\n",
    "\\frac{n-r(d)+1}{n}\n",
    "$$\n",
    "Where $n$ is the total number of documents in a rank list and $r(d)$ is the rank position of a document.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "af52e1c9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "map                      all     0.2697\n",
      "ndcg_cut_10              all     0.4370\n",
      "recall_1000              all     0.6881\n"
     ]
    }
   ],
   "source": [
    "def borda(runs):\n",
    "    seen = {}\n",
    "    for run in runs:\n",
    "        for topic, results in run.items():\n",
    "            # print(topic, results)\n",
    "            if topic not in seen:\n",
    "                seen[topic] = {}\n",
    "            # print(results.keys())\n",
    "            for i, docid in enumerate(results.keys()):\n",
    "                # print(i, docid)\n",
    "                n = len(results)\n",
    "                rd = i\n",
    "                score = (n-i+1)/n\n",
    "                if docid not in seen[topic]:\n",
    "                    seen[topic][docid] = score\n",
    "                else:\n",
    "                    seen[topic][docid] += score\n",
    "    return seen\n",
    "\n",
    "fused_borda_run = borda(runs)\n",
    "print_results(fused_borda_run)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8082bee9",
   "metadata": {},
   "source": [
    "### CombSUM\n",
    "\n",
    "CombSUM adds the retrieval scores of documents contained in more than one list and rearranges the order. It can be expressed formally as:\n",
    "$$\n",
    "\\sum_{d\\in D}s(d)\n",
    "$$\n",
    "Where $D$ is the set of documents (produced by the union of the runs) and $s(d)$ is the score of a single document."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "7c704416",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "map                      all     0.3237\n",
      "ndcg_cut_10              all     0.4457\n",
      "recall_1000              all     0.6881\n"
     ]
    }
   ],
   "source": [
    "def combsum(runs):\n",
    "    seen = {}\n",
    "    for run in runs:\n",
    "        for topic, results in run.items():\n",
    "            if topic not in seen:\n",
    "                seen[topic] = {}\n",
    "            for docid, score in results.items():\n",
    "                #TODO fill in the gap.\n",
    "                if docid not in seen[topic]:\n",
    "                    seen[topic][docid] = score\n",
    "                else:\n",
    "                    seen[topic][docid] += score\n",
    "                \n",
    "                \n",
    "    return seen\n",
    "\n",
    "fused_combsum_run = combsum(runs)\n",
    "print_results(fused_combsum_run)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e1bcaf6a",
   "metadata": {},
   "source": [
    "### CombMNZ\n",
    "\n",
    "CombMNZ adds the retrieval scores of documents contained in more than one list and rearranges the order, and multiplies their sum by the number of lists where the document occurs. It can be expressed formally as:\n",
    "$$\n",
    "|d\\in R > 0|\\cdot\\sum_{d\\in D}s(d)\n",
    "$$\n",
    "Where $D$ is the set of documents and $s(d)$ is the score of a single document, and $|d\\in R > 0|$ is the number of runs $R$ in which $d$ occurs with a score greater than zero."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "5c7b2ce6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "map                      all     0.3248\n",
      "ndcg_cut_10              all     0.4564\n",
      "recall_1000              all     0.6881\n"
     ]
    }
   ],
   "source": [
    "def combmnz(runs):\n",
    "    # TODO your implementation here.\n",
    "    seen = {}\n",
    "    num = {}\n",
    "    for run in runs:\n",
    "        count = []\n",
    "        for topic, results in run.items():\n",
    "            if topic not in seen:\n",
    "                seen[topic] = {}\n",
    "            for docid, score in results.items():\n",
    "                if docid not in count:\n",
    "                    count.append(docid)\n",
    "                if docid not in seen[topic]:\n",
    "                    seen[topic][docid] = score\n",
    "                else:\n",
    "                    seen[topic][docid] += score\n",
    "        for docid in count:\n",
    "            if docid not in num:\n",
    "                num[docid] = 1\n",
    "            else:\n",
    "                num[docid] += 1\n",
    "    \n",
    "    # print(num)\n",
    "    \n",
    "    for topic in seen:\n",
    "        for docid in seen[topic]:\n",
    "            seen[topic][docid] *= num[docid]\n",
    "    \n",
    "    return seen\n",
    "        \n",
    "\n",
    "fused_combmnz_run = combmnz(runs)\n",
    "print_results(fused_combmnz_run)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aee956f8",
   "metadata": {},
   "source": [
    "### Analysis\n",
    "\n",
    "Find the most effective combination of runs to fuse by trying all combinations, evaluating using MAP and nDCG, and performing statistical significance tests. Complete the table and questions below with the most effective combination only.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "75c1d4f3",
   "metadata": {},
   "source": [
    "|         | MAP  | nDCG |\n",
    "| ------- | ---- | ---- |\n",
    "| Borda   |0.2697|0.4370|\n",
    "| CombSUM |0.3237|0.4457|\n",
    "| CombMNZ |0.3248|0.4564|"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f7565bd3",
   "metadata": {},
   "source": [
    "_What is the most effective combination of runs to use, and which fusion method lead to this result?_ "
   ]
  },
  {
   "cell_type": "raw",
   "id": "9e6ed4c0",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "8862a9a3",
   "metadata": {},
   "source": [
    "### Questions\n",
    "\n",
    "Using all of the data you have collected about different combinations of runs with different fusion methods, complete the following questions.\n",
    "\n",
    "_What happens when you remove the normalisation of runs? Which fusion method(s) does this change? Why?_\n"
   ]
  },
  {
   "cell_type": "raw",
   "id": "3d4e3c8c",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "53ed9731",
   "metadata": {},
   "source": [
    "_Can you think of any other ways to fuse runs together? Use a publication database (e.g., Google Scholar) to search for research about \"rank fusion\". Summarise one (or more) other method(s) in the space provided below._"
   ]
  },
  {
   "cell_type": "raw",
   "id": "91d31306",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
